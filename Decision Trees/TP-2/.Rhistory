Z$Y = Z$Y - Y.mean
print(Z)
A <- cov(Z)
print(A)
# 3.2
L <- eigen(as.matrix(A))
L$values
# For ease of use
var_X = A[1, 1]
var_y = A[2, 2]
covariance = A[1, 2]
lambda1 = L$values[1]
lambda2 = L$values[2]
trace <- var_x + var_y
sum <- lambda1 + lambda2
# For ease of use
var_x = A[1, 1]
var_y = A[2, 2]
covariance = A[1, 2]
lambda1 = L$values[1]
lambda2 = L$values[2]
trace <- var_x + var_y
sum <- lambda1 + lambda2
print(trace)
print(trace == sum)
if (trace == sum) {
print("Equal")
} else {
print("Not equal")
}
# 3
# 3.1
X = c(0, 1, 0, 1, 1, 0)
Y = c(0, 1, 1, 0, 1, 0)
arr <- array(c(X,Y), dim = c(6,2,1))
print(arr)
# Poids statistiques:
Poids <- 1 / length(X)
# Matrice diagonale D des poids des individus
D <- diag(Poids, length(X), length(X))
print(D)
# Moyenne:
X.mean <- mean(X)
Y.mean <- mean(Y)
Z <- data.frame(X, Y)
Z$X = Z$X - X.mean
Z$Y = Z$Y - Y.mean
print(Z)
A <- cov(Z)
print(A)
# 3.2
L <- eigen(as.matrix(A))
L$values
# For ease of use
var_x = A[1, 1]
var_y = A[2, 2]
covariance = A[1, 2]
lambda1 = L$values[1]
lambda2 = L$values[2]
trace <- var_x + var_y
sum <- lambda1 + lambda2
if (trace == sum) {
print("Equal")
} else {
print("Not equal")
}
det <- var_x * var_y - covariance * covariance * covariance
mult <- lambda1 * lambda2
if (det == mult) {
print("Equal")
} else {
print("Not equal")
}
# 3.3
c1 = 1/sqrt(((var_y) - lambda1)^2 + (covariance)^2)
c2 = 1/sqrt(((var_y) - lambda2)^2 + (covariance)^2)
u1 <- -c1 * c(var_y - lambda1, -covariance)
u2 <- -c2 * c(var_y - lambda2, -covariance)
u1
u2
L$vectors
# 3.4
u = cbind(u1,u2)
Is = t(u) %*% A %*% u
Is
L$values
# 3.5
Is1 = t(u1) %*% A %*% u1
Is2 = t(u2) %*% A %*% u2
It = A[1, 1] + A[2, 2]
print(It)
print(Is1 + Is2)
# 3.6
plot(Z, pch=19)
abline(a=0, b=u1[2]/u1[1], col="blue")
abline(a=0, b=u2[2]/u2[1], col="red")
# 3.7
total <- L$values[1] + L$values[2]
TI1 <- L$values[1] / total
TI2 <- L$values[2] / total
TI1
TI2
# 4
GMi1 <- rep(0, nrow(Z))
for (i in 1:nrow(Z)) {
GMi1[i] = c(Z[i,1], Z[i,2]) %*% u1
}
GMi1
GMi2 <- rep(0, nrow(Z))
for (i in 1:nrow(Z)) {
GMi2[i] = c(Z[i,1], Z[i,2]) %*% u2
}
GMi2
GMi = cbind(GMi1, GMi2)
# 5
Qi = rep(0, nrow(Z))
for (i in 1:nrow(Z)) {
Qi[i] = sum(GMi^2)/sum(Z^2)
}
Qi
# 6
gammai_1 = rep(0, nrow(Z))
for (i in 1:nrow(Z)) {
gammai_1[i] = (1/nrow(Z)) * (GMi1[i]^2/L$values[1])
}
gammai_1
gammai_2 = rep(0, nrow(Z))
for (i in 1:nrow(Z)) {
gammai_2[i] = (1/nrow(Z)) * (GMi2[i]^2/L$values[2])
}
gammai_2
# TP
# Ce code prend une matrice de la forme p*n et fait les modifications necessaires
ACP_function <- function(matrice, norme) {
# Standardization
m_ce <- NULL
if (norme) {
for(i in 1:nrow(matrice)) {
m_ce <- rbind(m_ce,(matrice[i,] - mean(matrice[i,]))/(sd(matrice[i,])))
}
} else {
m_ce <- matrice - mean(matrice)
}
# Matrice de covariance
m_ce <- as.data.frame(t(m_ce))
m_cov <- cov(m_ce)
# Valeurs et vecteurs propres
vp <- eigen(m_cov)
vectorP <- vp$vectors
# ACP
m_ce_t <- t(m_ce)
var <- rep(0 , nrow(m_ce))
var <- t(vectorP) %*% m_ce_t
var <- as.data.frame(t(var))
print(var)
# Contributions
valeursP <- vp$values
percentage <- valeursP/(nrow(matrice)-1)
percentage <- round((percentage/sum(percentage))*100, 1)
return(var)
}
# Testing
df <-ACP_function(t(data), FALSE)
# Testing
data <- data.frame(X, Y)
df <- ACP_function(t(data), FALSE)
print("Taux d’inertie expliqué par le composante 1: " + TI1)
print(paste("Taux d’inertie expliqué par le composante 1:", TI1))
print(paste("Taux d’inertie expliqué par le composante 1:", TI1))
print(paste("Taux d’inertie expliqué par le composante 1:", TI2))
u1
print(paste("1ere composante:", u1))
print(paste("2eme composante:", u2))
library(rpart)
# Split data into train and test
set.seed(1234)
index <- sample(1:nrow(data), round(0.70*nrow(data)))
train <- data[index,]
test <- data[-index,]
nrow(train)
nrow(test)
# Make sure all categorical variables are of type `factor`
data <- read.csv("data.csv", sep = ";")
categorical_variables = c("marital", "education", "default", "housing", "loan", "contact", "poutcome", "class")
data[, categorical_variables] <- lapply(data[, categorical_variables], as.factor)
head(data)
# Exploratory data analysis
# Overall summary
summary(data)
# Search for any missing values
missing_count <- colSums(is.na(data))
print(missing_count) # => No missing data in our database
# Distributions of observations
hist_obj <- hist(data$balance, plot = FALSE)
hist(data$balance, main = "Histogram with Density Estimate", xlab = "Balance", ylab = "Frequency")
lines(density(data$balance), col = "blue", lwd = 2)
library(rpart)
# Split data into train and test
set.seed(1234)
index <- sample(1:nrow(data), round(0.70*nrow(data)))
train <- data[index,]
test <- data[-index,]
nrow(train)
# Make sure all categorical variables are of type `factor`
data <- read.csv("data.csv", sep = ";")
setwd("~/Uni/Saint-Etiennes/Semester 7/Data Science/UP2/Arbre de décision")
# Make sure all categorical variables are of type `factor`
data <- read.csv("data.csv", sep = ";")
categorical_variables = c("marital", "education", "default", "housing", "loan", "contact", "poutcome", "class")
data[, categorical_variables] <- lapply(data[, categorical_variables], as.factor)
head(data)
# Exploratory data analysis
# Overall summary
summary(data)
# Search for any missing values
missing_count <- colSums(is.na(data))
print(missing_count) # => No missing data in our database
# Distributions of observations
hist_obj <- hist(data$balance, plot = FALSE)
hist(data$balance, main = "Histogram with Density Estimate", xlab = "Balance", ylab = "Frequency")
lines(density(data$balance), col = "blue", lwd = 2)
library(rpart)
# Split data into train and test
set.seed(1234)
index <- sample(1:nrow(data), round(0.70*nrow(data)))
train <- data[index,]
test <- data[-index,]
nrow(train)
nrow(test)
cp_values <- seq(0.0001, 0.005, by = 0.0005)
# Initialize an empty vector to store cross-validation results
cv_results <- numeric(length(cp_values))
# Perform k-fold cross-validation
k <- 5
for (i in 1:length(cp_values)) {
cp <- cp_values[i]
# Set up cross-validation
set.seed(123)  # For reproducibility
folds <- sample(1:k, nrow(data), replace = TRUE)
# Initialize vector to store accuracy for each fold
fold_accuracies <- numeric(k)
for (j in 1:k) {
# Split the data into training and validation sets
validation_data <- data[folds == j, ]
training_data <- data[folds != j, ]
# Fit the decision tree model
tree_model <- rpart(class~., data = training_data, method = "class", cp = cp)
# Make predictions on the validation set
predictions <- predict(tree_model, newdata = validation_data, type = "class")
# Calculate accuracy for this fold
fold_accuracy <- mean(predictions == validation_data$class)
fold_accuracies[j] <- fold_accuracy
# Calculate other metrics
# xerror, xstd...
}
# Calculate the mean accuracy across folds
cv_results[i] <- mean(fold_accuracies)
}
# Create a data frame to store results
results_table <- data.frame(CP = cp_values, Mean_CV_Accuracy = cv_results)
# Print or inspect the results table
print(results_table)
cp_values <- seq(0, 0.005, by = 0.0005)
# Initialize an empty vector to store cross-validation results
cv_results <- numeric(length(cp_values))
# Perform k-fold cross-validation
k <- 5
for (i in 1:length(cp_values)) {
cp <- cp_values[i]
# Set up cross-validation
set.seed(123)  # For reproducibility
folds <- sample(1:k, nrow(data), replace = TRUE)
# Initialize vector to store accuracy for each fold
fold_accuracies <- numeric(k)
for (j in 1:k) {
# Split the data into training and validation sets
validation_data <- data[folds == j, ]
training_data <- data[folds != j, ]
# Fit the decision tree model
tree_model <- rpart(class~., data = training_data, method = "class", cp = cp)
# Make predictions on the validation set
predictions <- predict(tree_model, newdata = validation_data, type = "class")
# Calculate accuracy for this fold
fold_accuracy <- mean(predictions == validation_data$class)
fold_accuracies[j] <- fold_accuracy
# Calculate other metrics
# xerror, xstd...
}
# Calculate the mean accuracy across folds
cv_results[i] <- mean(fold_accuracies)
}
# Create a data frame to store results
results_table <- data.frame(CP = cp_values, Mean_CV_Accuracy = cv_results)
# Print or inspect the results table
print(results_table)
source("~/Uni/Saint-Etiennes/Semester 7/Data Science/UP2/Arbre de décision/Partie-II.R")
setwd("~/Uni/Saint-Etiennes/Semester 7/Data Science/UP2/Arbre de décision/TP-2")
library(rpart)
train <- read.csv("exemple_code.csv", sep=";")
test <- read.csv("mot_code.csv", sep=";")
View(train)
View(train)
View(train)
model <- rpart(y~., data = train, method="class")
predicted <- predict(model, test, type="class")
predicted <- ordered(predicted, levels = c(1, 0))
actual<- ordered(test$pgstat, levels = c(1, 0))
model <- rpart(y~., data = train, method="class")
rpart.plot::rpart.plot(model)
library(rpart)
rpart.plot::rpart.plot(model)
rpart.plot::rpart.plot(model)
printcp(fulltree)
printcp(model)
model$cptable
train_control <- trainControl(method = "cv", number = 5)
model <- train(y ~ ., data = train, method = "class", trControl = train_control)
train <- read.csv("exemple_code.csv", sep=";")
train_control <- trainControl(method = "cv", number = 5)
library(caret)
library(caret)
train <- read.csv("exemple_code.csv", sep=";")
test <- read.csv("mot_code.csv", sep=";")
train_control <- trainControl(method = "cv", number = 5)
model <- rpart(y~., data = train, method = "class", control = train_control)
model <- rpart(y~., data = train, method = "class")
for (i in 1:16) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
print(submatrix)
}
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
print(submatrix)
}
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predictions[i] <- predict(model, test, type="class")
}
predictions <- matrix(nrow = 16, ncol = 12)
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predictions[i] <- predict(model, test, type="class")
}
predicted <- predict(model, test, type="class")
predicted
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predicted <- predict(model, test, type="class")
predicted
}
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predictions[i] <- predict(model, submatrix, type="class")
}
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predicted <- predict(model, submatrix, type="class")
print(predicted)
}
predictions <- matrix(nrow = 1, ncol = 12)
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predicted <- predict(model, submatrix, type="class")
print(predicted)
}
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predictions[i] <- predict(model, submatrix, type="class")
print(predicted)
}
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predictions[i] <- predict(model, submatrix, type="class")
print(predicted)
}
predictions <- matrix(nrow = 10, ncol = 12)
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predictions[i] <- predict(model, submatrix, type="class")
print(predicted)
}
predictions <- matrix(nrow = 10, ncol = 12)
predictions
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predicted <- predict(model, submatrix, type="class")
}
predicted
predictions <- matrix(nrow = 10, ncol = 16)
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predictions[i] <- predict(model, submatrix, type="class")
}
predictions <- matrix(nrow = 10, ncol = 16)
predictions
predictions <- matrix(nrow = 10, ncol = 16)
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predicted <- predict(model, submatrix, type="class")
for (j in 1:16) {
predictions[i, j] <- predicted[j]
}
}
predictions
max_values <- apply(my_matrix, MARGIN = 2, FUN = max)
max_values <- apply(predictions, MARGIN = 2, FUN = max)
max_values
View(train)
model <- rpart(y~., data = train, method = "class")
predictions <- matrix(nrow = 10, ncol = 16)
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predicted <- predict(model, submatrix, type="class")
for (j in 1:16) {
predictions[i, j] <- predicted[j]
}
}
max_values <- apply(predictions, MARGIN = 2, FUN = max)
max_values
calculate_mode <- function(x) {
uniq_x <- unique(x)
uniq_x[which.max(tabulate(match(x, uniq_x)))]
}
# Calculate the mode for each column
mode_values <- apply(predictions, MARGIN = 2, FUN = calculate_mode)
max_values
library(rpart)
library(caret)
train <- read.csv("exemple_code.csv", sep=";")
train$y <- as.factor(train$y)
test <- read.csv("mot_code.csv", sep=";")
train_control <- trainControl(method = "cv", number = 5)
model <- rpart(y~., data = train, method = "class")
predictions <- matrix(nrow = 10, ncol = 16)
for (i in 1:10) {
start_row <- (i - 1) * 16 + 1
end_row <- i * 16
submatrix <- test[start_row:end_row, ]
predicted <- predict(model, submatrix, type="class")
for (j in 1:16) {
predictions[i, j] <- predicted[j]
}
}
calculate_mode <- function(x) {
uniq_x <- unique(x)
uniq_x[which.max(tabulate(match(x, uniq_x)))]
}
# Calculate the mode for each column
mode_values <- apply(predictions, MARGIN = 2, FUN = calculate_mode)
max_values
mode_values
level_to_class_mapping <- levels(train$y)
original_classes <- level_to_class_mapping[mode_values]
original_classes
word <- paste(original_classes, collapse = "")
word
# Split the number into pairs of digits
pairs <- substr(word, 1, nchar(word) - 1)
# 8060844860847620
encoding_scheme <- c("04" = "a", "08" = "b", "12" = "c", "16" = "d", "20" = "e",
"24" = "f", "28" = "g", "32" = "h", "36" = "i", "40" = "j",
"44" = "k", "48" = "l", "52" = "m", "56" = "n", "60" = "o",
"64" = "p", "68" = "q", "72" = "r", "76" = "s", "80" = "t",
"84" = "u", "88" = "v", "92" = "w", "96" = "x")
word <- paste(original_classes, collapse = "")
# Split the number into pairs of digits
pairs <- substr(word, 1, nchar(word) - 1)
# Map pairs to letters based on the encoding scheme
letters <- sapply(strsplit(pairs, ""), function(pair) encoding_scheme[paste0(pair, collapse = "")])
# Concatenate the letters to form the word
resulting_word <- paste(letters, collapse = "")
resulting_word
letters
pairs
# 8060844860847620
encoding_scheme <- c("04" = "a", "08" = "b", "12" = "c", "16" = "d", "20" = "e",
"24" = "f", "28" = "g", "32" = "h", "36" = "i", "40" = "j",
"44" = "k", "48" = "l", "52" = "m", "56" = "n", "60" = "o",
"64" = "p", "68" = "q", "72" = "r", "76" = "s", "80" = "t",
"84" = "u", "88" = "v", "92" = "w", "96" = "x")
word <- paste(original_classes, collapse = "")
word
